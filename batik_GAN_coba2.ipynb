{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNFWZa6xpI3Fv+XhYlbBYmd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FaridFK/pola-batik-baru/blob/main/batik_GAN_coba2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --no-cache-dir gdown"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JNFOUE_ZBUCJ",
        "outputId": "e995c16b-7c10-4fc8-ad70-5718bda221cb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (4.7.3)\n",
            "Collecting gdown\n",
            "  Downloading gdown-5.1.0-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.13.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2024.2.2)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Installing collected packages: gdown\n",
            "  Attempting uninstall: gdown\n",
            "    Found existing installation: gdown 4.7.3\n",
            "    Uninstalling gdown-4.7.3:\n",
            "      Successfully uninstalled gdown-4.7.3\n",
            "Successfully installed gdown-5.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "e4-zoceXAFs6"
      },
      "outputs": [],
      "source": [
        "import torch, torchvision, os, PIL, pdb\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import make_grid\n",
        "from tqdm.auto import tqdm\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def show(tensor, num=25, wandbactive=0, name=''):\n",
        "\n",
        "  # mengambil data tensor yang didetach terlebih dahulu dari gpu dan ditransfer ke cpu agar dapat ditampilkan oleh fungsi plt.imshow\n",
        "\n",
        "  data = tensor.detach().cpu()\n",
        "\n",
        "  # membuat grid dari data yang sudah diambil dari tensor, dengan baris 5.\n",
        "  # lalu dipermute(1, 2, 0) karena data dari tensor sebelumnya memiliki dimensi(batch_size, channel, height, width).\n",
        "  # setelah dipermute menjadi (heigh, width, channel). hasil dari permute membuat data dapat ditampilkan dengan benar menggunakan plt.imshow\n",
        "\n",
        "  grid = make_grid(data[:num], nrow=5).permute(1,2,0)\n",
        "\n",
        "  # optional\n",
        "  # apabila wandbact=1 mengirim grid ke wandb sesuai dengan parameter input name\n",
        "\n",
        "  if(wandbactive==1):\n",
        "    wandb.log({name:wandb.Image(grid.numpy().clip(0,1))})\n",
        "  plt.imshow(grid.clip(0,1))\n",
        "  plt.show()\n",
        "\n",
        "# hyperparameter & general parameter\n",
        "# jumlah epoch\n",
        "\n",
        "n_epochs = 10000\n",
        "\n",
        "# jumlah sample yang digunakan dalam satu iterasi learning\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "# learning rate = 0.0001\n",
        "\n",
        "lr = 1e-4\n",
        "\n",
        "# dimensi ruang laten yang dipakai dalam generator (jumlah fitur yang digunakan untuk mengambil sampel acak dari ruang laten)\n",
        "z_dim = 200\n",
        "\n",
        "device = 'cuda'\n",
        "cur_step = 0\n",
        "crit_cycles = 5\n",
        "gen_losses = []\n",
        "crit_losses = []\n",
        "show_step = 35\n",
        "save_step = 35\n",
        "wandbact = 1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# optional\n",
        "!pip install wandb -qqq\n",
        "import wandb\n",
        "wandb.login(key='444a4cc88c98ea16d3933f474a957b7a3e10390f')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LtD8ujY3BaYC",
        "outputId": "5119f457-a1ff-46c0-e98d-3a0ef2ec618e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m195.4/195.4 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.8/258.8 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "experiment_name = \"wgan\"\n",
        "myrun = wandb.init(\n",
        "    project = \"wgan\",\n",
        "    group = experiment_name,\n",
        "    config = {\n",
        "        \"optimizer\":\"adam\",\n",
        "        \"model\":\"wgan gp\",\n",
        "        \"epoch\":\"1000\",\n",
        "        \"batch_size\":128\n",
        "    }\n",
        ")\n",
        "config = wandb.config"
      ],
      "metadata": {
        "id": "pTY6TgvyBcWR"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(experiment_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-X0xDqv5BeQT",
        "outputId": "3057c0a3-809d-4d41-9fa2-172830ea9fac"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wgan\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.modules.activation import Tanh\n",
        "from torch.nn.modules.conv import Conv2d\n",
        "\n",
        "# generator model\n",
        "# mendefinisikan kelas generator yang mewarisi kelas nn.Module dari pytorch, digunakannya nn.Module dikarenakan mempunyai fungsi pembantu\n",
        "# yaitu forward propagation dan backward propagation, serta parameter2 untuk menyimpan nilai layer dalam model\n",
        "\n",
        "class Generator(nn.Module):\n",
        "\n",
        "  # mendefinisikan fungsi init sebagai konstruktor kelas generator dengan parameter z_dim dan d_dim\n",
        "  # z_dim : jumlah ruang laten yang digunakan sebagai input generator\n",
        "  # d_dim : d_dim adalah jumlah channel yang digunakan dalam setiap lapisan (layer)\n",
        "\n",
        "  def __init__(self, z_dim=64, d_dim=16):\n",
        "    super(Generator, self).__init__()\n",
        "    self.z_dim = z_dim\n",
        "\n",
        "    # diinisasi gen dengan nilai perhitungan melalui nn.Sequential\n",
        "    # nn.Sequential adalah modul yang berfungsi untuk mengelompokan modul modul menjadi rangkaian modul yang lebih besar.\n",
        "    # dimana modul yang dirangkai akan dieksekusi secara berurutan.\n",
        "    # dengan setiap modul akan menerima input dari modul sebelumnya dan menghasilkan -\n",
        "    # output yang akan digunakan sebagai input untuk modul berikutnya\n",
        "\n",
        "    self.gen = nn.Sequential(\n",
        "\n",
        "        # ConvTranspose2d : in_channels, out_channels, kernel_size, stride=1, padding=0\n",
        "        # calculating new width & height : (n - 1) * stride - 2 * padding + ks\n",
        "        # n : height & width, ks : kernel size\n",
        "        # begin with 1x1 image with z_dim number of channels (200)\n",
        "\n",
        "        # Conv2Transpose2d merupakan operasi yang digunakan untuk konvolusi transpose atau de-convolution pada tensor.\n",
        "        # modul ini memiliki beberapa parameter yaitu :\n",
        "        # in_channels : jumlah input channel\n",
        "        # out_channels : jumlah output channel\n",
        "        # kernel_size : ukuran kernel filter yang digunakan\n",
        "        # stride : jarak antar titik saat dilakukan convolution\n",
        "        # padding : jumlah padding yang ditambahkan pada setiap sisi tensor sebelum dilakukan convolution\n",
        "\n",
        "        # proses layer pertama nn.ConvTranspose2d dimulai dengan in_channels sebesar self.z_dim (ruang laten = 200 dimensi),\n",
        "        # out_channels sebesar d_dim * 32 (16x32=512), kernel_size sebesar 4, stride sebesar 1, dan padding sebesar 0,\n",
        "        # dari parameter tersebut menghasilkan 512 channel yang akan digunakan pada operasi layer berikutnya\n",
        "        # setelah melakukan perhitungan dilakukan proses nn.BatchNorm2d pada output channel yang akan dipakai pada layer berikutnya\n",
        "        # nn.BatchNorm2d adalah lapisan perhitungan untuk normalisasi data pada setiap batch,\n",
        "        # dengan menghitung mean dan standard deviation dari setiap channel pada data input,\n",
        "        # lalu data input dikalikan dengan standard deviation dan dikurangi dengan mean.\n",
        "        # dilakukannya normalisasi untuk mengontrol ukuran data input agar tidak terlalu besar atau terlalu kecil, training stabil dan cepat converge\n",
        "        # selanjutnya dilakukan proses aktivasi dengan nn.ReLU(true), yaitu untuk menambah non-linearitas pada model\n",
        "        # nilai yang lebih kecil dari 0 akan diubah menjadi 0 dengan input dari output nn.ConvTranspose2 sebelumnya,\n",
        "        # dan True berarti dilakukan secara inplace pada tensor tanpa membuat copy\n",
        "        # selanjutnya akan diulang proses pada tiap layer dengan parameter yang berbeda hingga menghasilkan output dengan ukutan 128x128 3 channel (RGB)\n",
        "\n",
        "        nn.ConvTranspose2d(z_dim, d_dim * 32, 4, 1, 0), # 4x4 (ch: 200, 512)\n",
        "        nn.BatchNorm2d(d_dim * 32),\n",
        "        nn.ReLU(True),\n",
        "\n",
        "        # dilakukan proses transpose konvolusi ulang dengan in_channel merupakan output dari out_channel sebelumnya.\n",
        "        # out_channel dideklarasikan dengan d_dim * 16 agar mendapatkan output 256 channel\n",
        "        # parameter kernel_size, stride dan padding di isi dengan 4, 2, 1 yang bertujuan untuk mengubah ukuran height dan width menjadi 8x8\n",
        "        # selanjutnya dilakukan normalisasi dan aktivasi seperti proses pada layer sebelumnya.\n",
        "\n",
        "        nn.ConvTranspose2d(d_dim * 32, d_dim * 16, 4, 2, 1), # 8x8 (ch: 512, 256)\n",
        "        nn.BatchNorm2d(d_dim * 16),\n",
        "        nn.ReLU(True),\n",
        "\n",
        "        # proses pada layer selanjutnya hampir sama dengan sebelumnya dengan mengubah parameter in dan out_channel agar menampilkan out 128\n",
        "        # dan pada parameter kernel_size, stride dan padding sama dengan sebelumny karena dapat menghasilkan height width baru yaitu 16x16\n",
        "\n",
        "        nn.ConvTranspose2d(d_dim * 16, d_dim * 8, 4, 2, 1), # 16x16 (ch: 256, 128)\n",
        "        nn.BatchNorm2d(d_dim * 8),\n",
        "        nn.ReLU(True),\n",
        "\n",
        "        # proses pada layer selanjutnya hampir sama dengan sebelumnya dengan mengubah parameter in dan out_channel agar menampilkan out 64\n",
        "        # dan pada parameter kernel_size, stride dan padding sama dengan sebelumny karena dapat menghasilkan height width baru yaitu 32x32\n",
        "\n",
        "        nn.ConvTranspose2d(d_dim * 8, d_dim * 4, 4, 2, 1), # 32x32 (ch: 128, 64)\n",
        "        nn.BatchNorm2d(d_dim * 4),\n",
        "        nn.ReLU(True),\n",
        "\n",
        "        # proses pada layer selanjutnya hampir sama dengan sebelumnya dengan mengubah parameter in dan out_channel agar menampilkan out 32\n",
        "        # dan pada parameter kernel_size, stride dan padding sama dengan sebelumny karena dapat menghasilkan height width baru yaitu 64x64\n",
        "\n",
        "        nn.ConvTranspose2d(d_dim * 4, d_dim * 2, 4, 2, 1), # 64x64 (ch: 64, 32)\n",
        "        nn.BatchNorm2d(d_dim * 2),\n",
        "        nn.ReLU(True),\n",
        "\n",
        "        # proses pada layer terakhir sedikit berbeda dengan sebelumnya dimana pada in_channel masih menggunakan out_channel layer sebelumnya\n",
        "        # sedangkan pada out_channel menggunakan nilai 3, hal ini bertujuan agar output channel sesuai dengan yang diinginkan yaitu RGB=3 chanel\n",
        "        # dan pada parameter kernel_size, stride dan padding sama dengan sebelumny karena dapat menghasilkan height width baru yaitu 128x128\n",
        "        # selanjutnya tidak dilakukan normalisasi maupun aktivasi melainkan fungsi nn.Tanh()\n",
        "        # nn.Tanh() adalah fungsi aktivasi akhir dari layer pada generator. fungsi ini digunakan untuk membatasi nilai output dari generator pada rentang -1 sampai 1\n",
        "        # agar output dari generator dapat diterima oleh fungsi loss yang digunakan dalam learning.\n",
        "        # nn.Tanh() cocok digunakan karena menghasilkan output yang berdisitribusi normal\n",
        "\n",
        "        nn.ConvTranspose2d(d_dim * 2, 3, 4, 2, 1), # 128x128 (ch: 32, 3)\n",
        "        nn.Tanh() # produce result in the range of -1 & 1\n",
        "    )\n",
        "\n",
        "  # dideklarasikan fungsi forward yang merupakan fungsi forward propagation turunan dari nn.Module kelas generator\n",
        "  # fungsi forward memiliki parameter noise, nantinya noise yang diinput akan diolah melalui proses Sequential sebelumnya\n",
        "  # dilakukan pengubahan bentuk tensor noise dari awalnya (batch_size, z_dim) menjadi (batch_size, z_dim, 1, 1)\n",
        "  # hal ini untuk menambahkan dimensi 1x1 untuk nantinya digunakan sebagi input pada proses konvolusi transpose nn.ConvTranspose2d di kelas generator\n",
        "  # selanjutnya di return untuk menjalakan forward propagation pada generator dengan input x sebelumnya.\n",
        "\n",
        "  def forward(self, noise):\n",
        "    x = noise.view(len(noise), self.z_dim, 1, 1) # structure : 128 x 200 x 1 x 1\n",
        "    return self.gen(x)\n",
        "\n",
        "# dideklarasikan fungsi gen_noise yg digunakan untuk menghasilkan data acak yg nantinya digunakan sebagai input fungsi forward\n",
        "# data acak ini disebut sebagai ruang laten (latent space) atau noise, fungsi ini memiliki parameter :\n",
        "# num : jumlah data acak yang akan dihasilkan\n",
        "# z_dim : jumlah dimensi pada ruang laten (latent space)\n",
        "# device : perangkat yang digunakan untuk menghasilkan data acak, defaultnya adalah cuda (GPU)\n",
        "# digunakan fungsi dari torch yaitu randn untuk menghasilkan data acak sesuai dengan jumlah dan dimensi yang sudah ditentukan,\n",
        "# lalu data acak tersebut akan diubah menjadi tensor dan dikirim ke device (gpu)\n",
        "\n",
        "def gen_noise(num, z_dim, device='cuda'):\n",
        "  return torch.randn(num, z_dim, device=device) # 128 batch size x 200 dimensionality"
      ],
      "metadata": {
        "id": "DUJ7vgM0Bf9H"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# n : height & width, stride : amount of sliding, padding : amount of edges, ks : kernel size (3x3)\n",
        "# nn.ConvTranspose2d : (n + 1) * stride - 2 * padding + ks\n",
        "# nn.Conv2d : (n + 2 * pad - ks) // stride + 1"
      ],
      "metadata": {
        "id": "I40roRLXBjA_"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# critic model\n",
        "\n",
        "# mendefinisikan kelas critic yang inherit nn.Module, bertugas mengevaluasi kevalidan dari suatu citra yang dihasilkan oleh generator\n",
        "\n",
        "class Critic(nn.Module):\n",
        "\n",
        "  # mendefinisikan fungsi init sebagai konstruktor kelas critic dengan hyperparametr self dan d_dim=16\n",
        "  # d_dim memiliki value 16, merupakan jumlah output channel pada layer pertama kelas critic\n",
        "  # output channels menentukan jumlah fitur yang diterima oleh layer setelah melalui proses konvolusi\n",
        "  # menjalankan super.init untuk menjalankan inisialisasi dari kelas parent (nn.Module)\n",
        "\n",
        "  def __init__(self, d_dim=16):\n",
        "    super(Critic, self).__init__()\n",
        "\n",
        "    # mendefinisikan arsitektur kelas critic menggunakan nn.Sequential yang memudahkan pembuatan arsitektur dari beberapa layer\n",
        "    # ada 4 layer Conv2d yang digunakan untuk melakukan konvolusi pada citra dan 1 layer Linear yang digunakan untuk melakukan prediksi skor diskriminator\n",
        "\n",
        "    self.crit = nn.Sequential(\n",
        "\n",
        "        # Conv2d : in_channels, out_channels, kernel_size, stride=1, padding=0, d_dim=16\n",
        "        # new width & height : (n + 2 * pad - ks) // stride +1\n",
        "\n",
        "        # Conv2d merupakakan suatu layer konvolusi untuk melakukan operasi konvolusi antara filter dengan matriks gambar\n",
        "        # Conv2d mengambil input gambar dan mengekstrak fitur yang mewakili gambar tersebut.\n",
        "        # berfungsi untuk mengkombinasikan entri-entri dari setiap filter dengan data input,\n",
        "        # dan memproduksi sebuah hasil konvolusi yang merupakan bagian dari layer baru. memiliki bbrp parametr yaitu:\n",
        "        # in_channels : jumlah input channel, diawal dinisasi 3 karena input gambar adalah RGB\n",
        "        # out_channels : jumlah output channel pada awal dinisasi dengan 16\n",
        "        # kernel_size : ukuran kernel / filter dalam operasi konvolusi, diawal 4\n",
        "        # stride : jarak antar titik saat dilakukan convolution\n",
        "        # padding : jumlah padding yang ditambahkan pada setiap sisi tensor sebelum dilakukan convolution\n",
        "\n",
        "        # pada layer pertama dilakukan Conv2d dengan parameternya dan menghasilkan 16 channel, serta width dan heigth baru yaitu 64x64\n",
        "        # sebelum digunakan pada layer selanjutnya, out_channel dinormalisasi menggunakan fungsi nn.InstanceNorm2d\n",
        "        # pada proses normalisasi, di hitung rata-rata dan deviasi standar dari setiap fitur (kanal) dalam set data,\n",
        "        # lalu membagi setiap nilai dalam fitur dengan deviasi standar dan menambahkan rata-rata.\n",
        "        # hal ini memastikan bahwa setiap fitur memiliki nilai yang sama besar dan membantu jaringan melakukan konvergensi lebih cepat.\n",
        "        # selanjutnya dilakukan aktivasi LeakyReLU yang menerima hasil dari lapisan konvolisi sebelumnya\n",
        "        # dan mengaktifkan fungsinya pada setiap pixel output. Fungsi ini membantu model untuk mempelajari fitur yang lebih rumit dalam gambar\n",
        "        # pada LeakyRelu, dilakukan \"leak\" untuk membiarkan masuknya beberapa input negatif sebagai tingkat kerugian input negatif\n",
        "        # pada kode ini, diisi value 0.2 yang brti jika input kurang dari 0, output akan menjadi 0.2 * input.\n",
        "        # jika input lebih besar dari 0, output akan menjadi input itu sendiri.\n",
        "\n",
        "        nn.Conv2d(3, d_dim, 4, 2, 1), # (128 + 2 * 1 - 4) // 2 +1 = 126 // 2 = 63 + 1 = 64 (ch: 3, 16) # 64x64\n",
        "        nn.InstanceNorm2d(d_dim),\n",
        "        nn.LeakyReLU(0.2),\n",
        "\n",
        "        # pada layer selanjutnya dilakukan proses yang sama yaitu convolusi dengan Conv2d, normalisasi dengan Instance dan aktivasi dengan LeakyRelu\n",
        "        # digunakan nilai output channel pada layer sebelumnya sebagai input channel dan menghasilkan wxh 32x32 serta out_channel 32.\n",
        "        # serta output channel dinaikan membantu untuk memperkuat model dan memberikan lebih banyak pilihan untuk mempelajari detail dari gambar.\n",
        "        # selain itu ini membantu memperkuat representasi dari setiap lapisan dan memastikan model memiliki informasi yang kuat untuk prediksi\n",
        "        # pada tiap layer ukuran terus menurun sedangkan channel terus naik hal ini karena model mempertimbangkan informasi pada skala besar,\n",
        "        # tidak hanya dari sebagian informasi saja\n",
        "        # sedangkan channel terus naik yg brrti model ditiap layer mempelajari fitur yang lebih kompleks dan abstrak dari citra asli.\n",
        "\n",
        "        nn.Conv2d(d_dim, d_dim*2, 4, 2, 1), # 32x32 (ch: 16, 32)\n",
        "        nn.InstanceNorm2d(d_dim*2),\n",
        "        nn.LeakyReLU(0.2),\n",
        "\n",
        "        # dilakukan proses yang sama layer dengan out_channel layer sebelumnya menjadi in channel serta menaikkan out channel menjadi 64\n",
        "        # menghasilkan wxh 16x16 dan out channel 64, dilakukan normalisasi pda out channel sebelum digunakan pada layer selanjutnya dan aktivasi juga.\n",
        "\n",
        "        nn.Conv2d(d_dim*2, d_dim*4, 4, 2, 1), # 16x16 (ch: 32, 64)\n",
        "        nn.InstanceNorm2d(d_dim*4),\n",
        "        nn.LeakyReLU(0.2),\n",
        "\n",
        "        # masih sama proses yang dilakukan hanya dirubah pada parameter in dan out channel\n",
        "\n",
        "        nn.Conv2d(d_dim*4, d_dim*8, 4, 2, 1), # 8x8 (ch: 64, 128)\n",
        "        nn.InstanceNorm2d(d_dim*8),\n",
        "        nn.LeakyReLU(0.2),\n",
        "        nn.Conv2d(d_dim*8, d_dim*16, 4, 2, 1), # 4x4 (ch: 128, 256)\n",
        "        nn.InstanceNorm2d(d_dim*16),\n",
        "        nn.LeakyReLU(0.2),\n",
        "\n",
        "        # pada layer terakhir memiliki parameter yang berbeda yaitu out channel 1 serta stride 1 dan padding 0\n",
        "        # out channel 1 karena untuk menentukan skor validitas gambar pada generator,\n",
        "        # output dari layer ini harus berupa skala satu dimensi karena merupakan skor validitas gambar dari generator.\n",
        "        # sedangkan stride 1 padding 0 untuk memastikan bahwa ukuran output dari layer ini sama dengan ukuran input,\n",
        "        # sehingga mudah untuk dicompare dengan target skor validitas gambar.\n",
        "\n",
        "        nn.Conv2d(d_dim*16, 1, 4, 1, 0) # (4 + 2 * 0 - 4) // 1 +1 = 1x1 (ch: 256, 1)\n",
        "    )\n",
        "\n",
        "  # didefinisikan fungsi forward dengan parameter image\n",
        "  # fungsi forward digunakan untuk melakukan forward propagation dari suatu input gambar melalui arsitektur yg didefinisikan sebelumnya.\n",
        "  # setiap layer yang ada dalam objek \"self.main\" akan diterapkan secara berurutan pada input \"image\"\n",
        "  # setelah melalui semua layer, output dari fungsi forward adalah tensor yang mewakili nilai prediksi dari kelas critic pada input gambar tersebut\n",
        "  # skor kevalidan gambar sebagai real atau buatan digunakan sebagai evaluasi\n",
        "\n",
        "  def forward(self, image):\n",
        "    # image : 128 (batch) x 3 (channel) x 128 x 128 (height and width)\n",
        "    crit_pred = self.crit(image) # 128 x 1 x 1 x 1\n",
        "    return crit_pred.view(len(crit_pred), -1) # 128 x 1"
      ],
      "metadata": {
        "id": "t7uTLUB2BkgY"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# optional, init weight in different way\n",
        "def init_weights(m):\n",
        "  if isintance(m, nn.Conv2d) or isintance(m, nn.ConvTranspose2d):\n",
        "    torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
        "    torch.nn.init.constant_(m.bias, 0)\n",
        "  if isintance(m, nn.BatchNorm2d):\n",
        "    torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
        "    torch.nn.init.constant_(m.bias, 0)\n",
        "# gen = gen.apply(init_weights)\n",
        "# crit = crit.apply(init_weights)"
      ],
      "metadata": {
        "id": "SL4m71iOBl_h"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import zipfile\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Path ke file zip di Google Drive\n",
        "zip_file_path = '/content/drive/MyDrive/batik_coba.zip'\n",
        "\n",
        "# Path tujuan untuk mengekstrak dataset\n",
        "extracted_path = '/content/batik'  # Ubah sesuai kebutuhan Anda\n",
        "\n",
        "# Mengekstrak file zip\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extracted_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxiGL5YPBorL",
        "outputId": "cc70182f-3cb9-48e7-d75e-b6c5b0ce8739"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import PIL\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Definisi kelas Dataset\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, path, size=128, limit=10000):\n",
        "        self.sizes = [size, size]\n",
        "        items, labels = [], []\n",
        "        for data in os.listdir(path)[:limit]:\n",
        "            # path: '/content/drive/MyDrive/path/to/your/dataset'\n",
        "            # data: 129880.JPG\n",
        "            item = os.path.join(path, data)\n",
        "            items.append(item)\n",
        "            labels.append(data)\n",
        "        self.items = items\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.items)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        data = PIL.Image.open(self.items[idx]).convert('RGB') # 178x128\n",
        "        data = np.asarray(torchvision.transforms.Resize(self.sizes)(data)) # 128x128x3 (resized)\n",
        "        data = np.transpose(data, (2, 0, 1)).astype(np.float32, copy=False) # 3x128x128 from 0 to 255\n",
        "        data = torch.from_numpy(data).div(255) # from 0 to 1\n",
        "        return data, self.labels[idx]\n",
        "\n",
        "# Path ke folder dataset di Google Drive\n",
        "data_path = '/content/batik/vz7pzt2grf-1'\n",
        "\n",
        "# Inisialisasi objek dataset\n",
        "ds = CustomDataset(data_path, size=128, limit=10000)\n",
        "\n",
        "# DataLoader\n",
        "batch_size = 64\n",
        "dataloader = DataLoader(ds, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Model generator dan kritikus\n",
        "gen = Generator(z_dim).to(device)\n",
        "crit = Critic().to(device)\n",
        "\n",
        "# Optimizer\n",
        "lr = 0.0002\n",
        "gen_opt = torch.optim.Adam(gen.parameters(), lr=lr, betas=(0.5, 0.9))\n",
        "crit_opt = torch.optim.Adam(crit.parameters(), lr=lr, betas=(0.5, 0.9))\n",
        "\n",
        "# Inisialisasi\n",
        "if wandbact == 1:\n",
        "    wandb.watch(gen, log_freq=100)\n",
        "    wandb.watch(crit, log_freq=100)\n",
        "\n",
        "# Menampilkan contoh data dari DataLoader\n",
        "x, y = next(iter(dataloader))\n",
        "show(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "1TZYINMDBqST",
        "outputId": "83ea3e46-5f50-47ba-c6f2-f826cc60c942"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-5f98bbb3bd43>\u001b[0m in \u001b[0;36m<cell line: 46>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;31m# Model generator dan kritikus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m \u001b[0mgen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0mcrit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCritic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1158\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1160\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m     def register_full_backward_pre_hook(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 810\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 810\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    831\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1156\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m   1157\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m-> 1158\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"LAZY\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# gradient penalty calculation\n",
        "def get_gp(real, fake, crit, alpha, gamma=10):\n",
        "  mix_images = real * alpha + fake * (1-alpha) # 128x3x128x128\n",
        "  mix_scores = crit(mix_images) # 128x1\n",
        "  gradient = torch.autograd.grad(\n",
        "      inputs = mix_images,\n",
        "      outputs = mix_scores,\n",
        "      grad_outputs = torch.ones_like(mix_scores),\n",
        "      retain_graph = True,\n",
        "      create_graph = True\n",
        "  )[0] #128x3x128x128\n",
        "  gradient = gradient.view(len(gradient), -1) #128x49152\n",
        "  gradient_norm = gradient.norm(2, dim=1)\n",
        "  gp = gamma * ((gradient_norm-1) ** 2).mean()\n",
        "  return gp"
      ],
      "metadata": {
        "id": "D781GaoLBsAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save & load checkpoints\n",
        "root_path = './data/'\n",
        "def save_checkpoint(name):\n",
        "  #gen\n",
        "  torch.save({\n",
        "      'epoch': epoch,\n",
        "      'model_state_dict': gen.state_dict(),\n",
        "      'optimizer_state_dict': gen_opt.state_dict()\n",
        "  }, f\"{root_path}G-{name}.pkl\")\n",
        "  #crit\n",
        "  torch.save({\n",
        "      'epoch': epoch,\n",
        "      'model_state_dict': crit.state_dict(),\n",
        "      'optimizer_state_dict': crit_opt.state_dict()\n",
        "  }, f\"{root_path}C-{name}.pkl\")\n",
        "  # print(\"checkpoint saved successfully\")\n",
        "\n",
        "def load_checkpoint(name):\n",
        "  #gen\n",
        "  checkpoint = torch.load(f\"{root_path}G-{name}.pkl\")\n",
        "  gen.load_state_dict(checkpoint['model_state_dict'])\n",
        "  gen_opt.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "  #crit\n",
        "  checkpoint = torch.load(f\"{root_path}C-{name}.pkl\")\n",
        "  crit.load_state_dict(checkpoint['model_state_dict'])\n",
        "  crit_opt.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "  print(\"checkpoint loaded successfully\")\n"
      ],
      "metadata": {
        "id": "wG-zvjghBtkp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen"
      ],
      "metadata": {
        "id": "gILTcfUpBvdJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# epoch=1\n",
        "# save_checkpoint(\"test\")"
      ],
      "metadata": {
        "id": "XNs7M6mkBxOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training loop\n",
        "for epoch in range(n_epochs):\n",
        "  for real, _ in tqdm(dataloader):\n",
        "    cur_bs = len(real) #128\n",
        "    real = real.to(device)\n",
        "    # critic\n",
        "    mean_crit_loss = 0\n",
        "    for _ in range(crit_cycles):\n",
        "      crit_opt.zero_grad()\n",
        "      noise = gen_noise(cur_bs, z_dim)\n",
        "      fake = gen(noise)\n",
        "      crit_fake_pred = crit(fake.detach())\n",
        "      crit_real_pred = crit(real)\n",
        "      # calculate gradient penalty\n",
        "      alpha = torch.rand(len(real), 1, 1, 1, device=device, requires_grad=True) # 128x1x1x1\n",
        "      gp = get_gp(real, fake.detach(), crit, alpha)\n",
        "      crit_loss = crit_fake_pred.mean() - crit_real_pred.mean() + gp\n",
        "      mean_crit_loss += crit_loss.item() / crit_cycles\n",
        "      crit_loss.backward(retain_graph=True)\n",
        "      crit_opt.step()\n",
        "    crit_losses += [mean_crit_loss]\n",
        "    # generator\n",
        "    gen_opt.zero_grad()\n",
        "    noise = gen_noise(cur_bs, z_dim)\n",
        "    fake = gen(noise)\n",
        "    crit_fake_pred = crit(fake)\n",
        "    gen_loss = -crit_fake_pred.mean()\n",
        "    gen_loss.backward()\n",
        "    gen_opt.step()\n",
        "    gen_losses += [gen_loss.item()]\n",
        "\n",
        "    # stats\n",
        "    if(wandbact==1):\n",
        "      wandb.log({'Epoch': epoch, 'Step': cur_step, 'Critic loss': mean_crit_loss, 'Gen loss': gen_loss})\n",
        "\n",
        "    if cur_step % save_step and cur_step > 0:\n",
        "      print(\"saving checkpoint\", cur_step, save_step)\n",
        "      save_checkpoint(\"latest\")\n",
        "\n",
        "    if(cur_step % show_step == 0 and cur_step > 0):\n",
        "      show(fake, wandbactive=1, name='Fake')\n",
        "      show(real, wandbactive=1, name='Real')\n",
        "\n",
        "      gen_mean = sum(gen_losses[-show_step:]) / show_step\n",
        "      crit_mean = sum(crit_losses[-show_step:]) / show_step\n",
        "      print(f\"Epoch: {epoch}: Step {cur_step}: Generator loss {gen_mean}, Critic loss {crit_mean}\")\n",
        "\n",
        "      plt.plot(\n",
        "          range(len(gen_losses)),\n",
        "          torch.Tensor(gen_losses),\n",
        "          label = \"Generator loss\"\n",
        "      )\n",
        "      plt.plot(\n",
        "          range(len(crit_losses)),\n",
        "          torch.Tensor(crit_losses),\n",
        "          label = \"Critic loss\"\n",
        "      )\n",
        "\n",
        "      plt.ylim(-1000, 1000)\n",
        "      plt.legend()\n",
        "      plt.show()\n",
        "\n",
        "    cur_step += 1\n"
      ],
      "metadata": {
        "id": "w0EoWpQ4Bysx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate\n",
        "noise = gen_noise(batch_size, z_dim)\n",
        "fake = gen(noise)\n",
        "show(fake)"
      ],
      "metadata": {
        "id": "yHQgMx5-B2HZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(fake[4].detach().cpu().permute(1,2,0).squeeze().clip(0,1))"
      ],
      "metadata": {
        "id": "51BmTID7B4RQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# morphing, interpolating between point in latent space\n",
        "from mpl_toolkits.axes_grid1 import ImageGrid\n",
        "\n",
        "# MORPHING, interpolation between points in latent space\n",
        "gen_set=[]\n",
        "z_shape=[1,200,1,1]\n",
        "rows=4\n",
        "steps=17\n",
        "\n",
        "for i in range(rows):\n",
        "  z1,z2 = torch.randn(z_shape), torch.randn(z_shape)\n",
        "  for alpha in np.linspace(0,1,steps):\n",
        "    z=alpha*z1 + (1-alpha)*z2\n",
        "    res=gen(z.cuda())[0]\n",
        "    gen_set.append(res)\n",
        "\n",
        "fig = plt.figure(figsize=(25,11))\n",
        "grid=ImageGrid(fig, 111, nrows_ncols=(rows,steps), axes_pad=0.1)\n",
        "\n",
        "for ax , img in zip (grid, gen_set):\n",
        "  ax.axis('off')\n",
        "  res=img.cpu().detach().permute(1,2,0)\n",
        "  res=res-res.min()\n",
        "  res=res/(res.max()-res.min())\n",
        "  ax.imshow(res.clip(0,1.0))\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xsPRnCLTB52g"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}